{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ead2d3e-1534-4e5f-befb-7c384a9abdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86cfefc4-a9d3-4398-8ff7-8e6056f2a3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seeds and environment variables set for reproducibility.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Set a fixed seed value\n",
    "seed_value = 10\n",
    "\n",
    "# 1. Set PYTHONHASHSEED environment variable for reproducibility in hashing\n",
    "os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "\n",
    "# 2. Set the seed for Python's built-in random module\n",
    "random.seed(10)\n",
    "\n",
    "# 3. Set the seed for NumPy\n",
    "np.random.seed(10)\n",
    "\n",
    "# 4. Set the seed for PyTorch\n",
    "torch.manual_seed(seed_value)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "# 5. Force PyTorch to use deterministic algorithms (may impact performance)\n",
    "torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "\n",
    "# 6. Optionally limit the number of threads used by OMP and MKL (helps reduce non-determinism)\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "print(\"Seeds and environment variables set for reproducibility.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65a1c266-f42d-48d1-9c4f-fc78f84224af",
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_series = nba_series = pd.read_csv(\"TEAM_MATCHUP_DATA_CLUSTER.csv\") # updated data set with CLUSTERING PROPORTIONS (CHECK BOX)\n",
    "\n",
    "nba_series['SEASON_YEAR'] = nba_series['SEASON'].str.split('-').str[0].astype(int)\n",
    "nba_series['SEASON'] = (nba_series['SEASON_YEAR'] + 1).astype(int)\n",
    "nba_series = nba_series.drop(columns=['SEASON_YEAR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82626607-6def-4a6c-8ada-d0a5e3a21c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sarvesh\\AppData\\Local\\Temp\\ipykernel_20820\\651144542.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  swapped['TEAM_1_W'] = (swapped['SERIES_WINNER'] == swapped['TEAM_1']).astype(int)\n"
     ]
    }
   ],
   "source": [
    "# Drop columns that are not needed\n",
    "nba_series = nba_series.drop(columns = ['SERIES_ID', 'SEASON_ID', 'TEAM_1_ID', 'TEAM_2_ID', 'CLUSTER_TEAM_1', 'CLUSTER_TEAM_2'])\n",
    "\n",
    "# Create a mask to flip half of the rows\n",
    "flip_mask = np.random.rand(len(nba_series)) < 0.5\n",
    "\n",
    "# Columns to swap\n",
    "team1_stat_cols = [col for col in nba_series.columns if '_TEAM_1' in col]\n",
    "team2_stat_cols = [col.replace('_TEAM_1', '_TEAM_2') for col in team1_stat_cols]\n",
    "\n",
    "# Include team name columns for flipping\n",
    "stat_swap_cols = team1_stat_cols + team2_stat_cols + ['TEAM_1', 'TEAM_2']\n",
    "\n",
    "# Create deep copies of swapped and non-swapped rows\n",
    "swapped = nba_series.loc[flip_mask].copy()\n",
    "not_swapped = nba_series.loc[~flip_mask].copy()\n",
    "\n",
    "# Flip stats\n",
    "swapped[team1_stat_cols] = nba_series.loc[flip_mask, team2_stat_cols].values\n",
    "swapped[team2_stat_cols] = nba_series.loc[flip_mask, team1_stat_cols].values\n",
    "\n",
    "# Flip team names\n",
    "swapped['TEAM_1'] = nba_series.loc[flip_mask, 'TEAM_2'].values\n",
    "swapped['TEAM_2'] = nba_series.loc[flip_mask, 'TEAM_1'].values\n",
    "\n",
    "# Recalculate TEAM_1_W based on new TEAM_1 vs SERIES_WINNER\n",
    "swapped['TEAM_1_W'] = (swapped['SERIES_WINNER'] == swapped['TEAM_1']).astype(int)\n",
    "not_swapped['TEAM_1_W'] = (not_swapped['SERIES_WINNER'] == not_swapped['TEAM_1']).astype(int)\n",
    "\n",
    "# Combine flipped and unflipped\n",
    "nba_series_balanced = pd.concat([swapped, not_swapped], ignore_index=True)\n",
    "\n",
    "# Optional: Shuffle the final DataFrame\n",
    "nba_series_balanced = nba_series_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# test_data_preserved is created for displaying results later on\n",
    "test_data_preserved = nba_series_balanced[['SEASON', 'TEAM_1', 'TEAM_2', 'SERIES_WINNER']]\n",
    "test_data_preserved = test_data_preserved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4e3e28e-d2dd-461c-bb48-01dc6d7304b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically extract stat bases by checking for matching suffixes\n",
    "diff_df = pd.DataFrame()\n",
    "\n",
    "# Grab all columns ending in _TEAM_1\n",
    "team1_cols = [col for col in nba_series_balanced.columns if col.endswith('_TEAM_1')]\n",
    "\n",
    "for col1 in team1_cols:\n",
    "    # Get the base stat name (e.g., 'AST', 'FG_PCT')\n",
    "    stat_base = col1.replace('_TEAM_1', '')\n",
    "    col2 = f'{stat_base}_TEAM_2'\n",
    "    \n",
    "    # Only compute diff if TEAM_2 version exists\n",
    "    if col2 in nba_series_balanced.columns:\n",
    "        diff_df[f'{stat_base}_DIFF'] = nba_series_balanced[col1] - nba_series_balanced[col2]\n",
    "\n",
    "# Add label and season columns\n",
    "diff_df['TEAM_1_W'] = nba_series_balanced['TEAM_1_W']\n",
    "diff_df['SEASON'] = nba_series_balanced['SEASON']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd896b9-1163-4321-998e-389dd371dbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "# Ensure save directory exists\n",
    "os.makedirs(\"saved_models\", exist_ok=True)\n",
    "\n",
    "years = np.arange(2020, 2025)\n",
    "\n",
    "def objective_tabnet(trial):\n",
    "    \n",
    "    n_d = trial.suggest_int(\"n_d\", 8, 64, step=8)\n",
    "    n_a = trial.suggest_int(\"n_a\", 8, 64, step=8)\n",
    "    n_steps = trial.suggest_int(\"n_steps\", 3, 10)\n",
    "    gamma = trial.suggest_float(\"gamma\", 1.0, 2.0)\n",
    "    lambda_sparse = trial.suggest_float(\"lambda_sparse\", 0.00001, 0.01, log=True)\n",
    "    momentum = trial.suggest_float(\"momentum\", 0.01, 0.4)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-3, 2e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "    mask_type = trial.suggest_categorical(\"mask_type\", ['sparsemax', 'entmax'])\n",
    "    \n",
    "\n",
    "    '''\n",
    "    n_d = trial.suggest_int(\"n_d\", 40, 56, step=4)\n",
    "    n_a = trial.suggest_int(\"n_a\", 40, 56, step=4)\n",
    "    n_steps = trial.suggest_int(\"n_steps\", 6, 10)\n",
    "    gamma = trial.suggest_float(\"gamma\", 1.0, 1.2)\n",
    "    lambda_sparse = trial.suggest_float(\"lambda_sparse\", 1e-5, 1e-3, log=True)\n",
    "    momentum = trial.suggest_float(\"momentum\", 0.3, 0.4)\n",
    "    lr = trial.suggest_float(\"lr\", 0.001, 0.003, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-5, log=True)\n",
    "    mask_type = trial.suggest_categorical(\"mask_type\", ['sparsemax', 'entmax'])\n",
    "    '''\n",
    "\n",
    "    # Store predictions and true labels for global metrics\n",
    "    all_y_true = []\n",
    "    all_y_pred = []\n",
    "    all_y_prob = []\n",
    "\n",
    "    for year in years:\n",
    "        train_data = diff_df[diff_df['SEASON'] < year]\n",
    "        test_data = diff_df[diff_df['SEASON'] == year]\n",
    "\n",
    "        if train_data.empty or test_data.empty:\n",
    "            continue\n",
    "\n",
    "        X_train = train_data.drop(columns=['TEAM_1_W', 'SEASON'])\n",
    "        y_train = train_data['TEAM_1_W']\n",
    "        X_test = test_data.drop(columns=['TEAM_1_W', 'SEASON'])\n",
    "        y_test = test_data['TEAM_1_W']\n",
    "\n",
    "        \n",
    "\n",
    "        smote = SMOTE(random_state=10)\n",
    "        X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "        clf = TabNetClassifier(\n",
    "            n_d=n_d, n_a=n_a, n_steps=n_steps,\n",
    "            gamma=gamma, lambda_sparse=lambda_sparse,\n",
    "            momentum=momentum, mask_type=mask_type,\n",
    "            optimizer_params={\"lr\": lr, \"weight_decay\": weight_decay},\n",
    "            seed=10, verbose=0, device_name='cpu'\n",
    "        )\n",
    "\n",
    "        clf.fit(\n",
    "            X_train_resampled.values, y_train_resampled.values,\n",
    "            eval_set=[(X_test.values, y_test.values)],\n",
    "            eval_metric=['auc'],\n",
    "            max_epochs=200, patience=20,\n",
    "            batch_size=64, virtual_batch_size=32\n",
    "        )\n",
    "\n",
    "        X_test_flip = -X_test.values\n",
    "        y_probs_flip = clf.predict_proba(X_test_flip)[:, 1]\n",
    "\n",
    "        # Symmetric inference: average regular and 1 - flipped\n",
    "        symmetric_probs = (clf.predict_proba(X_test.values)[:, 1] + (1 - y_probs_flip)) / 2\n",
    "        y_pred = (symmetric_probs >= 0.5).astype(int)\n",
    "\n",
    "        #y_probs = clf.predict_proba(X_test.values)[:, 1]\n",
    "        #y_pred = (y_probs >= 0.5).astype(int)\n",
    "\n",
    "        #preds_label = clf.predict(X_test.values)\n",
    "\n",
    "        # Collect for global metrics\n",
    "        all_y_true.extend(y_test.values)\n",
    "        all_y_pred.extend(y_pred)\n",
    "        all_y_prob.extend(symmetric_probs)\n",
    "\n",
    "    if len(all_y_true) == 0:\n",
    "        # Skip trial if no data\n",
    "        return 0.0\n",
    "\n",
    "    # Compute global metrics\n",
    "    global_auc = roc_auc_score(all_y_true, all_y_prob)\n",
    "    global_accuracy = accuracy_score(all_y_true, all_y_pred)\n",
    "    global_f1 = f1_score(all_y_true, all_y_pred)\n",
    "\n",
    "    print(f\"Trial {trial.number}: Global AUC = {global_auc:.4f}, Accuracy = {global_accuracy:.4f}, F1 = {global_f1:.4f}\")\n",
    "\n",
    "    \n",
    "    # ✅ Save models with high AUC\n",
    "    if global_auc >= 0.90:\n",
    "        filename = f\"tabnet_trial{trial.number}_auc{global_auc:.4f}.zip\"\n",
    "        filepath = os.path.join(\"saved_models\", filename)\n",
    "        clf.save_model(filepath)\n",
    "        print(f\"✅ Saved model for Trial {trial.number} → {filename}\")\n",
    "    \n",
    "\n",
    "    return global_auc\n",
    "\n",
    "# Run the Optuna study\n",
    "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=8))\n",
    "study.optimize(objective_tabnet, n_trials=200)\n",
    "\n",
    "# Print best results\n",
    "print(\"Best AUC:\", study.best_value)\n",
    "print(\"Best Params:\", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b502659a-0ee6-4641-b5a8-a37ee86aed06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📆 Evaluating year: 2020\n",
      "\n",
      "Early stopping occurred at epoch 46 with best_epoch = 26 and best_val_0_auc = 0.875\n",
      "\n",
      "Early stopping occurred at epoch 48 with best_epoch = 28 and best_val_0_auc = 0.92857\n",
      "AUC: 0.8393 | Accuracy: 0.8000 | F1: 0.8235\n",
      "Binary Cross-Entropy Loss: 0.5730\n",
      "Confusion Matrix:\n",
      "[[5 2]\n",
      " [1 7]]\n",
      "\n",
      "📆 Evaluating year: 2021\n",
      "\n",
      "Early stopping occurred at epoch 30 with best_epoch = 10 and best_val_0_auc = 0.88889\n",
      "\n",
      "Early stopping occurred at epoch 41 with best_epoch = 21 and best_val_0_auc = 0.7963\n",
      "AUC: 0.8889 | Accuracy: 0.8667 | F1: 0.8571\n",
      "Binary Cross-Entropy Loss: 0.5945\n",
      "Confusion Matrix:\n",
      "[[7 2]\n",
      " [0 6]]\n",
      "\n",
      "📆 Evaluating year: 2022\n",
      "\n",
      "Early stopping occurred at epoch 29 with best_epoch = 9 and best_val_0_auc = 0.92593\n",
      "\n",
      "Early stopping occurred at epoch 48 with best_epoch = 28 and best_val_0_auc = 0.96296\n",
      "AUC: 1.0000 | Accuracy: 0.8667 | F1: 0.8750\n",
      "Binary Cross-Entropy Loss: 0.4970\n",
      "Confusion Matrix:\n",
      "[[6 0]\n",
      " [2 7]]\n",
      "\n",
      "📆 Evaluating year: 2023\n",
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 8 and best_val_0_auc = 0.96429\n",
      "\n",
      "Early stopping occurred at epoch 22 with best_epoch = 2 and best_val_0_auc = 0.89286\n",
      "AUC: 0.9464 | Accuracy: 0.9333 | F1: 0.9231\n",
      "Binary Cross-Entropy Loss: 0.4906\n",
      "Confusion Matrix:\n",
      "[[8 0]\n",
      " [1 6]]\n",
      "\n",
      "📆 Evaluating year: 2024\n",
      "\n",
      "Early stopping occurred at epoch 35 with best_epoch = 15 and best_val_0_auc = 0.94\n",
      "\n",
      "Early stopping occurred at epoch 36 with best_epoch = 16 and best_val_0_auc = 1.0\n",
      "AUC: 0.9200 | Accuracy: 0.8667 | F1: 0.8889\n",
      "Binary Cross-Entropy Loss: 0.5184\n",
      "Confusion Matrix:\n",
      "[[5 0]\n",
      " [2 8]]\n",
      "\n",
      "🔁 Overall Blended Model Performance:\n",
      "AUC: 0.9057, Accuracy: 0.8667, F1: 0.8718, Precision: 0.8947, Recall: 0.8500\n",
      "Overall Binary Cross-Entropy Loss: 0.5347\n"
     ]
    }
   ],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, confusion_matrix, precision_score, recall_score, log_loss\n",
    "import numpy as np\n",
    "\n",
    "# Tracking\n",
    "all_y_true = []\n",
    "all_y_prob_blended = []\n",
    "all_y_pred_blended = []\n",
    "\n",
    "# Per-year metrics\n",
    "yearly_metrics = []\n",
    "\n",
    "years = np.arange(2020, 2025)\n",
    "\n",
    "# 84% accuracy set (2020-2025 test), 0.883 AUC, threshold = 0.5, seed = 8 (fit, optuna)\n",
    "params_a = {\n",
    "    'n_d': 56,\n",
    "    'n_a': 64,\n",
    "    'n_steps': 6,\n",
    "    'gamma': 1.3930814539055605,\n",
    "    'lambda_sparse': 0.0029833605183872134,\n",
    "    'momentum': 0.15891701764952307,\n",
    "    'mask_type': 'entmax'\n",
    "}\n",
    "\n",
    "opt_params_a = {\n",
    "    'lr': 0.0076661428463103455,\n",
    "    'weight_decay': 2.501056095511525e-05\n",
    "}\n",
    "\n",
    "# 85% accuracy set (2020-2025 test), 0.89 AUC, threshold = 0.51, seed = 10 (fit, optuna)\n",
    "params_b = {\n",
    "    'n_d': 48,\n",
    "    'n_a': 48,\n",
    "    'n_steps': 8,\n",
    "    'gamma': 1.0950455922432034,\n",
    "    'lambda_sparse': 0.000365814411562923,\n",
    "    'momentum': 0.3675391603570311,\n",
    "    'mask_type': 'sparsemax'\n",
    "}\n",
    "\n",
    "opt_params_b = {\n",
    "    'lr': 0.002071649487926403,\n",
    "    'weight_decay': 4.2583833531711615e-06\n",
    "}\n",
    "\n",
    "# 79% accuracy set (2005-2005 test), 0.869 AUC, threshold = 0.5, seed = 5 (fit, optuna) \n",
    "params_c = {\n",
    "    'n_d': 48,\n",
    "    'n_a': 32,\n",
    "    'n_steps': 5,\n",
    "    'gamma': 1.1885687695822824,\n",
    "    'lambda_sparse': 0.0019929526182699384,\n",
    "    'momentum': 0.25773242436863386,\n",
    "    'mask_type': 'entmax'\n",
    "}\n",
    "\n",
    "opt_params_c = {\n",
    "    'lr': 0.01601995480535204,\n",
    "    'weight_decay': 0.00015417059849810502,\n",
    "}\n",
    "\n",
    "\n",
    "for year in years:\n",
    "    print(f\"\\n📆 Evaluating year: {year}\")\n",
    "    train_data = diff_df[diff_df['SEASON'] < year]\n",
    "    test_data = diff_df[diff_df['SEASON'] == year]\n",
    "    \n",
    "    if train_data.empty or test_data.empty:\n",
    "        continue\n",
    "\n",
    "    X_train = train_data.drop(columns=['TEAM_1_W', 'SEASON'])\n",
    "    y_train = train_data['TEAM_1_W']\n",
    "    X_test = test_data.drop(columns=['TEAM_1_W', 'SEASON'])\n",
    "    y_test = test_data['TEAM_1_W']\n",
    "\n",
    "    smote = SMOTE(random_state=10)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    # -------- Model A --------\n",
    "    model_a = TabNetClassifier(\n",
    "        **params_a,\n",
    "        optimizer_params=opt_params_a,\n",
    "        seed=8,\n",
    "        device_name='cpu',\n",
    "        verbose=0\n",
    "    )\n",
    "    model_a.fit(\n",
    "        X_train_resampled.values, y_train_resampled.values,\n",
    "        eval_set=[(X_test.values, y_test.values)],\n",
    "        eval_metric=['auc'],\n",
    "        max_epochs=200, patience=20,\n",
    "        batch_size=64, virtual_batch_size=32\n",
    "    )\n",
    "    probs_a = model_a.predict_proba(X_test.values)[:, 1]\n",
    "    \n",
    "    \n",
    "    # -------- Model B --------\n",
    "    model_b = TabNetClassifier(\n",
    "        **params_b,\n",
    "        optimizer_params=opt_params_b,\n",
    "        seed=10,\n",
    "        device_name='cpu',\n",
    "        verbose=0\n",
    "    )\n",
    "    model_b.fit(\n",
    "        X_train_resampled.values, y_train_resampled.values,\n",
    "        eval_set=[(X_test.values, y_test.values)],\n",
    "        eval_metric=['auc'],\n",
    "        max_epochs=200, patience=20,\n",
    "        batch_size=64, virtual_batch_size=32\n",
    "    )\n",
    "    probs_b = model_b.predict_proba(X_test.values)[:, 1]\n",
    "    \n",
    "    '''\n",
    "    # -------- Model C --------\n",
    "    model_c = TabNetClassifier(\n",
    "        **params_c,\n",
    "        optimizer_params=opt_params_c,\n",
    "        seed=5,\n",
    "        device_name='cpu',\n",
    "        verbose=0\n",
    "    )\n",
    "    model_c.fit(\n",
    "        X_train_resampled.values, y_train_resampled.values,\n",
    "        eval_set=[(X_test.values, y_test.values)],\n",
    "        eval_metric=['auc'],\n",
    "        max_epochs=200, patience=20,\n",
    "        batch_size=64, virtual_batch_size=32\n",
    "    )\n",
    "    probs_c = model_c.predict_proba(X_test.values)[:, 1]\n",
    "    '''\n",
    "    \n",
    "    X_test_flipped = -X_test.values\n",
    "\n",
    "    '''\n",
    "    # -------- Blend Predictions --------\n",
    "    blended_probs = (probs_a + probs_b) / 2\n",
    "    blended_preds = (blended_probs >= 0.505).astype(int)\n",
    "    '''\n",
    "\n",
    "    # Flip all the features by negating them\n",
    "    X_test_flipped = -X_test.values\n",
    "\n",
    "    # Get predictions for flipped inputs\n",
    "    probs_a_flip = model_a.predict_proba(X_test_flipped)[:, 1]\n",
    "    probs_b_flip = model_b.predict_proba(X_test_flipped)[:, 1]\n",
    "    #probs_c_flip = model_c.predict_proba(X_test_flipped)[:, 1]\n",
    "    \n",
    "    # Blend forward and flipped predictions separately\n",
    "    blended_forward = (probs_a + probs_b) / 2\n",
    "    blended_flipped = 1 - ((probs_a_flip + probs_b_flip) / 2)  # invert because flipped input reverses label\n",
    "\n",
    "    # Final symmetric probability: average of regular and flipped\n",
    "    symmetric_probs = (blended_forward + blended_flipped) / 2\n",
    "\n",
    "    # Convert to binary predictions using threshold\n",
    "    blended_preds = (symmetric_probs >= 0.52).astype(int)\n",
    "    blended_probs = symmetric_probs\n",
    "\n",
    "    bce_loss = log_loss(y_test, blended_probs)\n",
    "\n",
    "    # Metrics for this year\n",
    "    auc = roc_auc_score(y_test, blended_probs)\n",
    "    acc = accuracy_score(y_test, blended_preds)\n",
    "    f1 = f1_score(y_test, blended_preds)\n",
    "    cm = confusion_matrix(y_test, blended_preds)\n",
    "\n",
    "    print(f\"AUC: {auc:.4f} | Accuracy: {acc:.4f} | F1: {f1:.4f}\")\n",
    "    print(f\"Binary Cross-Entropy Loss: {bce_loss:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    yearly_metrics.append({\n",
    "        \"year\": year,\n",
    "        \"auc\": auc,\n",
    "        \"accuracy\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"confusion_matrix\": cm\n",
    "    })\n",
    "\n",
    "    # Global tracking\n",
    "    all_y_true.extend(y_test)\n",
    "    all_y_prob_blended.extend(blended_probs)\n",
    "    all_y_pred_blended.extend(blended_preds)\n",
    "\n",
    "# ------ Overall Metrics ------\n",
    "overall_auc = roc_auc_score(all_y_true, all_y_prob_blended)\n",
    "overall_acc = accuracy_score(all_y_true, all_y_pred_blended)\n",
    "overall_f1 = f1_score(all_y_true, all_y_pred_blended)\n",
    "overall_prec = precision_score(all_y_true, all_y_pred_blended)\n",
    "overall_rec = recall_score(all_y_true, all_y_pred_blended)\n",
    "overall_bce_loss = log_loss(all_y_true, all_y_prob_blended)\n",
    "\n",
    "print(f\"\\n🔁 Overall Blended Model Performance:\")\n",
    "print(f\"AUC: {overall_auc:.4f}, Accuracy: {overall_acc:.4f}, F1: {overall_f1:.4f}, Precision: {overall_prec:.4f}, Recall: {overall_rec:.4f}\")\n",
    "print(f\"Overall Binary Cross-Entropy Loss: {overall_bce_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (NBACluster)",
   "language": "python",
   "name": "nbacluster"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
